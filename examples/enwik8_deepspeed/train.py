# -*- coding: utf-8 -*-
"""audio_preprocessing_blocksparse_routing_deepspeed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lzokgqvm0vpinCZqH71-6UOZiHvD047E

## **Initialization**
"""

import argparse
import numpy       as np
import tensorflow as tf
# import blocksparse as bs
from mpi4py import MPI

import fnmatch
import os
import random
import re
import threading
import librosa,librosa.display
import numpy as npx
import tensorflow as tf
from six.moves import xrange
import better_exceptions
import tensorflow as tf
import numpy as np
import os
import time
import json
import numpy as np
import av
import torch as t
import tqdm
import soundfile
import matplotlib.pyplot as plt
from torch.utils.data import Dataset,DataLoader
from torch.utils.data.distributed import DistributedSampler
from time import sleep
import torch.nn as nn
import torch.nn.functional as F
import math

import math
import pickle
import numpy as np
import torch as t
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader
from torch.autograd import Variable
import torch.distributions as dist
import sys
import os
import random
import re
import threading
import librosa,librosa.display
from six.moves import xrange
import torch.optim as optim
# from torch.utils.tensorboard import SummaryWriter
from argparse import ArgumentParser
import tqdm,gzip
import tqdm
import deepspeed
from routing_transformer import RoutingTransformerLM
from routing_transformer.autoregressive_wrapper import AutoregressiveWrapper

def audio_preprocess(x, hps):
    # Extra layer in case we want to experiment with different preprocessing
    # For two channel, blend randomly into mono (standard is .5 left, .5 right)

    # x: NTC
    x = x.float()
    if x.shape[-1]==2:
        if hps.aug_blend:
            mix=t.rand((x.shape[0],1), device=x.device) #np.random.rand()
        else:
            mix = 0.5
        x=(mix*x[:,:,0]+(1-mix)*x[:,:,1])
    elif x.shape[-1]==1:
        x=x[:,:,0]
    else:
        assert False, f'Expected channels {hps.channels}. Got unknown {x.shape[-1]} channels'

    # x: NT -> NTC
    x = x.unsqueeze(2)
    return x

def audio_postprocess(x, hps):
    return x

def get_duration_sec(file, cache=False):
    try:
        with open(file + '.dur', 'r') as f:
            duration = float(f.readline().strip('\n'))
        return duration
    except:
        container = av.open(file)
        audio = container.streams.get(audio=0)[0]
        duration = audio.duration * float(audio.time_base)
        if cache:
            with open(file + '.dur', 'w') as f:
                f.write(str(duration) + '\n')
        return duration

def load_audio(file, sr, offset, duration, resample=True, approx=False, time_base='samples', check_duration=True):
    if time_base == 'sec':
        offset = offset * sr
        duration = duration * sr
    # Loads at target sr, stereo channels, seeks from offset, and stops after duration
    container = av.open(file)
    audio = container.streams.get(audio=0)[0] # Only first audio stream
    audio_duration = audio.duration * float(audio.time_base)
    if approx:
        if offset + duration > audio_duration*sr:
            # Move back one window. Cap at audio_duration
            offset = np.min(audio_duration*sr - duration, offset - duration)
    else:
        if check_duration:
            assert offset + duration <= audio_duration*sr, f'End {offset + duration} beyond duration {audio_duration*sr}'
    if resample:
        resampler = av.AudioResampler(format='fltp',layout='stereo', rate=sr)
    else:
        assert sr == audio.sample_rate
    offset = int(offset / sr / float(audio.time_base)) #int(offset / float(audio.time_base)) # Use units of time_base for seeking
    duration = int(duration) #duration = int(duration * sr) # Use units of time_out ie 1/sr for returning
    sig = np.zeros((2, duration), dtype=np.float32)
    container.seek(offset, stream=audio)
    total_read = 0
    for frame in container.decode(audio=0): # Only first audio stream
        if resample:
            frame.pts = None
            frame = resampler.resample(frame)
        frame = frame.to_ndarray(format='fltp') # Convert to floats and not int16
        read = frame.shape[-1]
        if total_read + read > duration:
            read = duration - total_read
        sig[:, total_read:total_read + read] = frame[:, :read]
        total_read += read
        if total_read == duration:
            break
    assert total_read <= duration, f'Expected {duration} frames, got {total_read}'
    return sig, sr

"""## **Simple Loader**"""

class AudioDataset(Dataset):
  def  __init__(self):
    #data loading
    self.sr = 11000
    self.channels = 2
    self.min_duration = 17.0
    self.max_duration = 30.0
    self.sample_length = 12.0
    self.aug_shift = False
    self.init_dataset()


  def get_index_offset(self, item):
    # For a given dataset item and shift, return song index and offset within song
    half_interval = self.sample_length//2
    shift = np.random.randint(-half_interval, half_interval) if self.aug_shift else 0
    offset = item * self.sample_length + shift # Note we centred shifts, so adding now
    midpoint = offset + half_interval
    assert 0 <= midpoint < self.cumsum[-1], f'Midpoint {midpoint} of item beyond total length {self.cumsum[-1]}'
    index = np.searchsorted(self.cumsum, midpoint)  # index <-> midpoint of interval lies in this song
    start, end = self.cumsum[index - 1] if index > 0 else 0.0, self.cumsum[index] # start and end of current song
    assert start <= midpoint <= end, f"Midpoint {midpoint} not inside interval [{start}, {end}] for index {index}"
    if offset > end - self.sample_length: # Going over song
        offset = max(start, offset - half_interval)  # Now should fit
    elif offset < start: # Going under song
        offset = min(end - self.sample_length, offset + half_interval)  # Now should fit
    #assert start <= offset <= end - self.sample_length, f"Offset {offset} not in [{start}, {end - self.sample_length}]. End: {end}, SL: {self.sample_length}, Index: {index}"
    if not(start <= offset and offset <= end - self.sample_length):
      offset = end - self.sample_length
    offset = offset - start
    return index, offset

  def init_dataset(self):
    files = librosa.util.find_files('./drive/My Drive/VQVAE-trans/dataset/2008/', ['mp3', 'm4a', 'opus','wav'])
    print(f'Found {len(files)} files!')
    files = files[:30]
    self.files = files 
    self.durations = [int(get_duration_sec(file)) for file in files]
    self.cumsum = np.cumsum(self.durations)

  def __getitem__(self,item):
    index, offset = self.get_index_offset(item)
    filename, total_length = self.files[index], self.durations[index]
    data, sr = load_audio(filename, sr=self.sr, offset=offset, duration=self.sample_length,time_base='sec')
    assert data.shape == (self.channels, self.sample_length*self.sr), f'Expected {(self.channels, self.sample_length)}, got {data.shape}'
    return data.T

  def __len__(self):
    return int(np.floor(self.cumsum[-1] / self.sample_length))

from tqdm import tqdm

def get_batch(file, loader):
  y1, sr = loader(file, sr=2200, offset=0.0, duration=32.0, time_base='sec')
  #y2, sr = loader(file, sr=44100, offset=20.0, duration=6.0, time_base='sec')
  
  # Some plotting for test purposes  
  # librosa.display.waveplot(y1,sr)
  # plt.xlabel('Time')
  # plt.ylabel('Amplitude')
  # plt.show()

  #Imp Note the first dimension of signal denotes channels if present
  # print(y1.shape)
  return y1

def load(file, loader):
  batch = get_batch(file, loader)  # np
  x = collate_fn(batch)  # torch cpu
  x = x.to('cuda', non_blocking=True)  # torch gpu
  return x


#using librosa to directly load audio file might consider in future!
# files = librosa.util.find_files('/content/drive/My Drive/audio_VAE/', ['mp3', 'm4a', 'opus'])

# signal,sr = librosa.load(files[0],sr=44100,offset=0.0,duration=6.0,mono=False)
# librosa.display.waveplot(signal,sr = sr)
# plt.xlabel('Time')
# plt.ylabel('Amplitude')
# plt.show()

# signal = np.array(signal)
# print(signal.shape)
##################################################################



# print(files[:10])

# loader = load_audio

# print("Loader", loader.__name__)
# # x = t.randn(2, 2).cuda()
# # x = load(files[0], loader)


# for i,file in enumerate(tqdm(files)):
#   # x = t.randn(2, 2).cuda()
#   x = load(file, loader)
#   if i == 100:
#       break

# print('some shapes')
# print(x.shape)
# print(x.shape[:-1])
# print(x.shape[-1])

collate_fn = lambda batch: t.stack([t.from_numpy(b) for b in batch], dim=0)

dataset = AudioDataset()

def save_object(obj, filename):
   with open(filename, 'wb') as output:  # Overwrites any existing file.
       pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)

print("Length of dataset is ", len(dataset))

train_loader = DataLoader(dataset, batch_size=1, num_workers=2, pin_memory=False,shuffle = True,drop_last=True, collate_fn=collate_fn)

def load_object(filename):
    with open(filename, 'rb') as input:  # Overwrites any existing file.
        dataset_reload = pickle.load(input)
    
    return dataset_reload

def save_object(obj, filename):
    with open(filename, 'wb') as output:  # Overwrites any existing file.
        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)

collate_fn = lambda batch: t.stack([t.from_numpy(b) for b in batch], dim=0)

def load_object(filename):
    with open(filename, 'rb') as input:  # Overwrites any existing file.
        dataset_reload = pickle.load(input)
    
    return dataset_reload

# dataset_reload = AudioDataset()

save_object(dataset, './drive/My Drive/VQVAE-trans/dataset/loader_routing.pkl')

dataset_reload = load_object('./drive/My Drive/VQVAE-trans/dataset/loader_routing.pkl')

print("Length of dataset is ", len(dataset_reload))

train_loader = DataLoader(dataset_reload, batch_size=1, num_workers=2, pin_memory=False,shuffle = True,drop_last=True, collate_fn=collate_fn)

dataset_reload[0].shape

"""## **VQ-VAE**"""

import torch.distributed as dist
from enum import Enum

class ReduceOp(Enum):
    SUM = 0,
    PRODUCT = 1,
    MIN = 2,
    MAX = 3

    def ToDistOp(self):
        return {
            self.SUM: dist.ReduceOp.SUM,
            self.PRODUCT: dist.ReduceOp.PRODUCT,
            self.MIN: dist.ReduceOp.MIN,
            self.MAX: dist.ReduceOp.MAX
        }[self]

def is_available():
    return dist.is_available()

def get_rank():
    if is_available():
        return _get_rank()
    else:
        return 0

def get_world_size():
    if is_available():
        return _get_world_size()
    else:
        return 1

def barrier():
    if is_available():
        return _barrier()
    #else: do nothing

def all_gather(tensor_list, tensor):
    if is_available():
        return _all_gather(tensor_list, tensor)
    else:
        tensor_list[0] = tensor

def all_reduce(tensor, op=ReduceOp.SUM):
    if is_available():
        return _all_reduce(tensor, op)
    #else: do nothing

def reduce(tensor, dst, op=ReduceOp.SUM):
    if is_available():
        return _reduce(tensor, dst, op)
    #else: do nothing

def broadcast(tensor, src):
    if is_available():
        return _broadcast(tensor, src)
    #else: do nothing

def init_process_group(backend, init_method):
    if is_available():
        return _init_process_group(backend, init_method)
    #else: do nothing

def _get_rank():
    return dist.get_rank()

def _barrier():
    return dist.barrier()

def _get_world_size():
    return dist.get_world_size()

def _all_gather(tensor_list, tensor):
    return dist.all_gather(tensor_list, tensor)

def _all_reduce(tensor, op):
    return dist.all_reduce(tensor, op.ToDistOp())

def _reduce(tensor, dst, op):
    return dist.reduce(tensor, dst, op.ToDistOp())

def _broadcast(tensor, src):
    return dist.broadcast(tensor, src)

def _init_process_group(backend, init_method):
    return dist.init_process_group(backend, init_method)


import gc

def freeze_model(model):
    model.eval()
    for params in model.parameters():
        params.requires_grad = False


def unfreeze_model(model):
    model.train()
    for params in model.parameters():
        params.requires_grad = True

def zero_grad(model):
    for p in model.parameters():
        if p.requires_grad and p.grad is not None:
            p.grad = None

def empty_cache():
    gc.collect()
    t.cuda.empty_cache()

def assert_shape(x, exp_shape):
    # assert x.shape == exp_shape, f"Expected {exp_shape} got {x.shape}"
    return

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def count_state(model):
    return sum(s.numel() for s in model.state_dict().values())

# Simple gradient checkpointing. Works with distributed data parallel

def checkpoint(func, inputs, params, flag):
    if flag:
        args = inputs + tuple(params)
        return CheckpointFunction.apply(func, len(inputs), *args)
    else:
        return func(*inputs)

class CheckpointFunction(t.autograd.Function):
    @staticmethod
    def forward(ctx, run_function, length, *args):
        ctx.run_function = run_function
        ctx.input_tensors = list(args[:length])
        ctx.input_params = list(args[length:])
        with t.no_grad():
            output_tensors = ctx.run_function(*ctx.input_tensors)
        return output_tensors

    @staticmethod
    def backward(ctx, *output_grads):
        for i in range(len(ctx.input_tensors)):
            temp = ctx.input_tensors[i]
            ctx.input_tensors[i] = temp.detach()
            ctx.input_tensors[i].requires_grad = temp.requires_grad
        with t.enable_grad():
            output_tensors = ctx.run_function(*ctx.input_tensors)
        input_grads = t.autograd.grad(output_tensors, ctx.input_tensors + ctx.input_params, output_grads, allow_unused=True)
        del ctx.input_tensors
        del output_tensors
        return (None, None) + input_grads

import numpy as np
import torch as t
import torch.nn as nn
import torch.nn.functional as F

class BottleneckBlock(nn.Module):
    def __init__(self, k_bins, emb_width, mu):
        super().__init__()
        self.k_bins = k_bins
        self.emb_width = emb_width
        self.mu = mu
        self.reset_k()
        self.threshold = 1.0

    def reset_k(self):
        self.init = False
        self.k_sum = None
        self.k_elem = None
        self.register_buffer('k', t.zeros(self.k_bins, self.emb_width).cuda())

    def _tile(self, x):
        d, ew = x.shape
        if d < self.k_bins:
            n_repeats = (self.k_bins + d - 1) // d
            std = 0.01 / np.sqrt(ew)
            x = x.repeat(n_repeats, 1)
            x = x + t.randn_like(x) * std
        return x

    def init_k(self, x):
        mu, emb_width, k_bins = self.mu, self.emb_width, self.k_bins
        self.init = True
        # init k_w using random vectors from x
        y = self._tile(x)
        _k_rand = y[t.randperm(y.shape[0])][:k_bins]
        #dist.broadcast(_k_rand, 0)
        self.k = _k_rand
        assert self.k.shape == (k_bins, emb_width)
        self.k_sum = self.k
        self.k_elem = t.ones(k_bins, device=self.k.device)

    def restore_k(self, num_tokens=None, threshold=1.0):
        mu, emb_width, k_bins = self.mu, self.emb_width, self.k_bins
        self.init = True
        assert self.k.shape == (k_bins, emb_width)
        self.k_sum = self.k.clone()
        self.k_elem = t.ones(k_bins, device=self.k.device)
        if num_tokens is not None:
            expected_usage = num_tokens / k_bins
            self.k_elem.data.mul_(expected_usage)
            self.k_sum.data.mul_(expected_usage)
        self.threshold = threshold

    def update_k(self, x, x_l):
        mu, emb_width, k_bins = self.mu, self.emb_width, self.k_bins
        with t.no_grad():
            # Calculate new centres
            x_l_onehot = t.zeros(k_bins, x.shape[0], device=x.device)  # k_bins, N * L
            x_l_onehot.scatter_(0, x_l.view(1, x.shape[0]), 1)

            _k_sum = t.matmul(x_l_onehot, x)  # k_bins, w
            _k_elem = x_l_onehot.sum(dim=-1)  # k_bins
            y = self._tile(x)
            _k_rand = y[t.randperm(y.shape[0])][:k_bins]

            # dist.broadcast(_k_rand, 0)
            # dist.all_reduce(_k_sum)
            # dist.all_reduce(_k_elem)

            # Update centres
            old_k = self.k
            self.k_sum = mu * self.k_sum + (1. - mu) * _k_sum  # w, k_bins
            self.k_elem = mu * self.k_elem + (1. - mu) * _k_elem  # k_bins
            usage = (self.k_elem.view(k_bins, 1) >= self.threshold).float()
            self.k = usage * (self.k_sum.view(k_bins, emb_width) / self.k_elem.view(k_bins, 1)) \
                     + (1 - usage) * _k_rand
            _k_prob = _k_elem / t.sum(_k_elem)  # x_l_onehot.mean(dim=-1)  # prob of each bin
            entropy = -t.sum(_k_prob * t.log(_k_prob + 1e-8))  # entropy ie how diverse
            used_curr = (_k_elem >= self.threshold).sum()
            usage = t.sum(usage)
            dk = t.norm(self.k - old_k) / np.sqrt(np.prod(old_k.shape))
        return dict(entropy=entropy,
                    used_curr=used_curr,
                    usage=usage,
                    dk=dk)

    def preprocess(self, x):
        # NCT -> NTC -> [NT, C]
        x = x.permute(0, 2, 1).contiguous()
        x = x.view(-1, x.shape[-1])  # x_en = (N * L, w), k_j = (w, k_bins)

        if x.shape[-1] == self.emb_width:
            prenorm = t.norm(x - t.mean(x)) / np.sqrt(np.prod(x.shape))
        elif x.shape[-1] == 2 * self.emb_width:
            x1, x2 = x[...,:self.emb_width], x[...,self.emb_width:]
            prenorm = (t.norm(x1 - t.mean(x1)) / np.sqrt(np.prod(x1.shape))) + (t.norm(x2 - t.mean(x2)) / np.sqrt(np.prod(x2.shape)))

            # Normalise
            x = x1 + x2
        else:
            assert False, f"Expected {x.shape[-1]} to be (1 or 2) * {self.emb_width}"
        return x, prenorm

    def postprocess(self, x_l, x_d, x_shape):
        # [NT, C] -> NTC -> NCT
        N, T = x_shape
        x_d = x_d.view(N, T, -1).permute(0, 2, 1).contiguous()
        x_l = x_l.view(N, T)
        return x_l, x_d

    def quantise(self, x):
        # Calculate latent code x_l
        k_w = self.k.t()
        distance = t.sum(x ** 2, dim=-1, keepdim=True) - 2 * t.matmul(x, k_w) + t.sum(k_w ** 2, dim=0,
                                                                                            keepdim=True)  # (N * L, b)
        min_distance, x_l = t.min(distance, dim=-1)
        fit = t.mean(min_distance)
        return x_l, fit

    def dequantise(self, x_l):
        x = F.embedding(x_l, self.k)
        return x

    def encode(self, x):
        N, width, T = x.shape

        # Preprocess.
        x, prenorm = self.preprocess(x)

        # Quantise
        x_l, fit = self.quantise(x)

        # Postprocess.
        x_l = x_l.view(N, T)
        return x_l

    def decode(self, x_l):
        N, T = x_l.shape
        width = self.emb_width

        # Dequantise
        x_d = self.dequantise(x_l)

        # Postprocess
        x_d = x_d.view(N, T, width).permute(0, 2, 1).contiguous()
        return x_d

    def forward(self, x, update_k=True):
        N, width, T = x.shape

        # Preprocess
        x, prenorm = self.preprocess(x)

        # Init k if not inited
        if update_k and not self.init:
            self.init_k(x)

        # Quantise and dequantise through bottleneck
        x_l, fit = self.quantise(x)
        x_d = self.dequantise(x_l)

        # Update embeddings
        if update_k:
            update_metrics = self.update_k(x, x_l)
        else:
            update_metrics = {}

        # Loss
        commit_loss = t.norm(x_d.detach() - x) ** 2 / np.prod(x.shape)

        # Passthrough
        x_d = x + (x_d - x).detach()

        # Postprocess
        x_l, x_d = self.postprocess(x_l, x_d, (N,T))
        return x_l, x_d, commit_loss, dict(fit=fit,
                                           pn=prenorm,
                                           **update_metrics)


class Bottleneck(nn.Module):
    def __init__(self, l_bins, emb_width, mu, levels):
        super().__init__()
        self.levels = levels
        level_block = lambda level: BottleneckBlock(l_bins, emb_width, mu)
        self.level_blocks = nn.ModuleList()
        for level in range(self.levels):
            self.level_blocks.append(level_block(level))

    def encode(self, xs):
        zs = [level_block.encode(x) for (level_block, x) in zip(self.level_blocks, xs)]
        return zs

    def decode(self, zs, start_level=0, end_level=None):
        if end_level is None:
            end_level = self.levels
        xs_quantised = [level_block.decode(z) for (level_block, z) in zip(self.level_blocks[start_level:end_level], zs)]
        return xs_quantised

    def forward(self, xs):
        zs, xs_quantised, commit_losses, metrics = [], [], [], []
        for level in range(self.levels):
            level_block = self.level_blocks[level]
            x = xs[level]
            z, x_quantised, commit_loss, metric = level_block(x, update_k=self.training)
            zs.append(z)
            if not self.training:
                # Be extra paranoid and make sure the encoder weights can't
                # change from straight-through estimator
                x_quantised = x_quantised.detach()
            xs_quantised.append(x_quantised)
            commit_losses.append(commit_loss)
            if self.training:
                metrics.append(metric)
        return zs, xs_quantised, commit_losses, metrics

class NoBottleneckBlock(nn.Module):
    def restore_k(self):
        pass

class NoBottleneck(nn.Module):
    def __init__(self, levels):
        super().__init__()
        self.level_blocks = nn.ModuleList()
        self.levels = levels
        for level in range(levels):
            self.level_blocks.append(NoBottleneckBlock())

    def encode(self, xs):
        return xs

    def decode(self, zs, start_level=0, end_level=None):
        if end_level is None:
            end_level = self.levels
        return zs

    def forward(self, xs):
        zero = t.zeros(()).cuda()
        commit_losses = [zero for _ in range(self.levels)]
        metrics = [dict(entropy=zero, usage=zero, used_curr=zero, pn=zero, dk=zero) for _ in range(self.levels)]
        return xs, xs, commit_losses, metrics

import math
import torch.nn as nn

class ResConvBlock(nn.Module):
    def __init__(self, n_in, n_state):
        super().__init__()
        self.model = nn.Sequential(
            nn.ReLU(),
            nn.Conv2d(n_in, n_state, 3, 1, 1),
            nn.ReLU(),
            nn.Conv2d(n_state, n_in, 1, 1, 0),
        )

    def forward(self, x):
        return x + self.model(x)

class Resnet(nn.Module):
    def __init__(self, n_in, n_depth, m_conv=1.0):
        super().__init__()
        self.model = nn.Sequential(*[ResConvBlock(n_in, int(m_conv * n_in)) for _ in range(n_depth)])

    def forward(self, x):
        return self.model(x)

class ResConv1DBlock(nn.Module):
    def __init__(self, n_in, n_state, dilation=1, zero_out=False, res_scale=1.0):
        super().__init__()
        padding = dilation
        self.model = nn.Sequential(
            nn.ReLU(),
            nn.Conv1d(n_in, n_state, 3, 1, padding, dilation),
            nn.ReLU(),
            nn.Conv1d(n_state, n_in, 1, 1, 0),
        )
        if zero_out:
            out = self.model[-1]
            nn.init.zeros_(out.weight)
            nn.init.zeros_(out.bias)
        self.res_scale = res_scale

    def forward(self, x):
        return x + self.res_scale * self.model(x)

class Resnet1D(nn.Module):
    def __init__(self, n_in, n_depth, m_conv=1.0, dilation_growth_rate=1, dilation_cycle=None, zero_out=False, res_scale=False, reverse_dilation=False, checkpoint_res=False):
        super().__init__()
        def _get_depth(depth):
            if dilation_cycle is None:
                return depth
            else:
                return depth % dilation_cycle
        blocks = [ResConv1DBlock(n_in, int(m_conv * n_in),
                                 dilation=dilation_growth_rate ** _get_depth(depth),
                                 zero_out=zero_out,
                                 res_scale=1.0 if not res_scale else 1.0 / math.sqrt(n_depth))
                  for depth in range(n_depth)]
        if reverse_dilation:
            blocks = blocks[::-1]
        self.checkpoint_res = checkpoint_res
        if self.checkpoint_res == 1:
            if dist.get_rank() == 0:
                print("Checkpointing convs")
            self.blocks = nn.ModuleList(blocks)
        else:
            self.model = nn.Sequential(*blocks)

    def forward(self, x):
        if self.checkpoint_res == 1:
            for block in self.blocks:
                x = checkpoint(block, (x, ), block.parameters(), True)
            return x
        else:
            return self.model(x)

class EncoderConvBlock(nn.Module):
    def __init__(self, input_emb_width, output_emb_width, down_t,
                 stride_t, width, depth, m_conv,
                 dilation_growth_rate=1, dilation_cycle=None, zero_out=False,
                 res_scale=False):
        super().__init__()
        blocks = []
        filter_t, pad_t = stride_t * 2, stride_t // 2
        if down_t > 0:
            for i in range(down_t):
                block = nn.Sequential(
                    nn.Conv1d(input_emb_width if i == 0 else width, width, filter_t, stride_t, pad_t),    #filter_t is kernel size
                    Resnet1D(width, depth, m_conv, dilation_growth_rate, dilation_cycle, zero_out, res_scale),
                )
                blocks.append(block)
            block = nn.Conv1d(width, output_emb_width, 3, 1, 1)
            blocks.append(block)
        self.model = nn.Sequential(*blocks)

    def forward(self, x):
        return self.model(x)

class DecoderConvBock(nn.Module):
    def __init__(self, input_emb_width, output_emb_width, down_t,
                 stride_t, width, depth, m_conv, dilation_growth_rate=1, dilation_cycle=None, zero_out=False, res_scale=False, reverse_decoder_dilation=False, checkpoint_res=False):
        super().__init__()
        blocks = []
        if down_t > 0:
            filter_t, pad_t = stride_t * 2, stride_t // 2
            block = nn.Conv1d(output_emb_width, width, 3, 1, 1)
            blocks.append(block)
            for i in range(down_t):
                block = nn.Sequential(
                    Resnet1D(width, depth, m_conv, dilation_growth_rate, dilation_cycle, zero_out=zero_out, res_scale=res_scale, reverse_dilation=reverse_decoder_dilation, checkpoint_res=checkpoint_res),
                    nn.ConvTranspose1d(width, input_emb_width if i == (down_t - 1) else width, filter_t, stride_t, pad_t)  #filter_t is kernel size
                )
                blocks.append(block)
        self.model = nn.Sequential(*blocks)

    def forward(self, x):
        return self.model(x)

class Encoder(nn.Module):
    def __init__(self, input_emb_width, output_emb_width, levels, downs_t,
                 strides_t, **block_kwargs):
        super().__init__()
        self.input_emb_width = input_emb_width
        self.output_emb_width = output_emb_width
        self.levels = levels
        self.downs_t = downs_t
        self.strides_t = strides_t

        block_kwargs_copy = dict(**block_kwargs)
        if 'reverse_decoder_dilation' in block_kwargs_copy:
            del block_kwargs_copy['reverse_decoder_dilation']
        level_block = lambda level, down_t, stride_t: EncoderConvBlock(input_emb_width if level == 0 else output_emb_width,
                                                           output_emb_width,
                                                           down_t, stride_t,
                                                           **block_kwargs_copy)
        self.level_blocks = nn.ModuleList()
        iterator = zip(list(range(self.levels)), downs_t, strides_t)
        for level, down_t, stride_t in iterator:
            self.level_blocks.append(level_block(level, down_t, stride_t))

    def forward(self, x):
        N, T = x.shape[0], x.shape[-1]
        emb = self.input_emb_width
        assert_shape(x, (N, emb, T))
        xs = []

        # 64, 32, ...
        iterator = zip(list(range(self.levels)), self.downs_t, self.strides_t)
        for level, down_t, stride_t in iterator:
            level_block = self.level_blocks[level]
            x = level_block(x)
            emb, T = self.output_emb_width, T // (stride_t ** down_t)
            assert_shape(x, (N, emb, T))
            xs.append(x)

        return xs

class Decoder(nn.Module):
    def __init__(self, input_emb_width, output_emb_width, levels, downs_t,
                 strides_t, **block_kwargs):
        super().__init__()
        self.input_emb_width = input_emb_width
        self.output_emb_width = output_emb_width
        self.levels = levels

        self.downs_t = downs_t

        self.strides_t = strides_t

        level_block = lambda level, down_t, stride_t: DecoderConvBock(output_emb_width,
                                                          output_emb_width,
                                                          down_t, stride_t,
                                                          **block_kwargs)
        self.level_blocks = nn.ModuleList()
        iterator = zip(list(range(self.levels)), downs_t, strides_t)
        for level, down_t, stride_t in iterator:
            self.level_blocks.append(level_block(level, down_t, stride_t))

        self.out = nn.Conv1d(output_emb_width, input_emb_width, 3, 1, 1)

    def forward(self, xs, all_levels=True):
        if all_levels:
            assert len(xs) == self.levels
        else:
            assert len(xs) == 1
        x = xs[-1]
        N, T = x.shape[0], x.shape[-1]
        emb = self.output_emb_width
        assert_shape(x, (N, emb, T))

        # 32, 64 ...
        iterator = reversed(list(zip(list(range(self.levels)), self.downs_t, self.strides_t)))
        for level, down_t, stride_t in iterator:
            level_block = self.level_blocks[level]
            x = level_block(x)
            emb, T = self.output_emb_width, T * (stride_t ** down_t)
            assert_shape(x, (N, emb, T))
            if level != 0 and all_levels:
                x = x + xs[level - 1]

        x = self.out(x)
        return x

class DefaultSTFTValues:
    def __init__(self, hps):
        self.sr = 11000
        self.n_fft = 2048
        self.hop_length = 256
        self.window_size = 6 * self.hop_length

class STFTValues:
    def __init__(self, hps, n_fft, hop_length, window_size):
        self.sr = 11000
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.window_size = window_size

# not sure about this but used in loss decode!
def calculate_bandwidth(dataset, hps, duration=600):
    hps = DefaultSTFTValues(hps)
    n_samples = int(dataset.sr * duration)
    l1, total, total_sq, n_seen = 0.0, 0.0, 0.0, 0.0
    spec_norm_total, spec_nelem = 0.0, 0.0
    idx = 0
    while n_seen < n_samples:
        x = dataset[idx]
        if isinstance(x, (tuple, list)):
            x, y = x
        samples = x.astype(np.float64)
        stft = librosa.core.stft(np.mean(samples, axis=1), hps.n_fft, hop_length=hps.hop_length, win_length=hps.window_size)
        spec = np.absolute(stft)
        spec_norm_total += np.linalg.norm(spec)
        spec_nelem += 1
        n_seen += int(np.prod(samples.shape))
        l1 += np.sum(np.abs(samples))
        total += np.sum(samples)
        total_sq += np.sum(samples ** 2)
        idx += 1

    # if dist.is_available():
    #     from jukebox.utils.dist_utils import allreduce
    #     n_seen = allreduce(n_seen)
    #     total = allreduce(total)
    #     total_sq = allreduce(total_sq)
    #     l1 = allreduce(l1)
    #     spec_nelem = allreduce(spec_nelem)
    #     spec_norm_total = allreduce(spec_norm_total)

    mean = total / n_seen
    bandwidth = dict(l2 = total_sq / n_seen - mean ** 2,
                     l1 = l1 / n_seen,
                     spec = spec_norm_total / spec_nelem)
    print(bandwidth)
    return bandwidth
    
############################################### above are important values try to debug ##################################

def stft(sig, hps):
    return t.stft(sig, hps.n_fft, hps.hop_length, win_length=hps.window_size, window=t.hann_window(hps.window_size, device=sig.device))

def spec(x, hps):
    return t.norm(stft(x, hps), p=2, dim=-1)

def norm(x):
    return (x.view(x.shape[0], -1) ** 2).sum(dim=-1).sqrt()

def squeeze(x):
    if len(x.shape) == 3:
        assert x.shape[-1] in [1,2]
        x = t.mean(x, -1)
    if len(x.shape) != 2:
        raise ValueError(f'Unknown input shape {x.shape}')
    return x

def spectral_loss(x_in, x_out, hps):
    hps = DefaultSTFTValues(hps)
    spec_in = spec(squeeze(x_in.float()), hps)
    spec_out = spec(squeeze(x_out.float()), hps)
    return norm(spec_in - spec_out)

def multispectral_loss(x_in, x_out, hps):
    losses = []
    assert len(hps.multispec_loss_n_fft) == len(hps.multispec_loss_hop_length) == len(hps.multispec_loss_window_size)
    args = [hps.multispec_loss_n_fft,
            hps.multispec_loss_hop_length,
            hps.multispec_loss_window_size]
    for n_fft, hop_length, window_size in zip(*args):
        hps = STFTValues(hps, n_fft, hop_length, window_size)
        spec_in = spec(squeeze(x_in.float()), hps)
        spec_out = spec(squeeze(x_out.float()), hps)
        losses.append(norm(spec_in - spec_out))
    return sum(losses) / len(losses)

def spectral_convergence(x_in, x_out, hps, epsilon=2e-3):
    hps = DefaultSTFTValues(hps)
    spec_in = spec(squeeze(x_in.float()), hps)
    spec_out = spec(squeeze(x_out.float()), hps)

    gt_norm = norm(spec_in)
    residual_norm = norm(spec_in - spec_out)
    mask = (gt_norm > epsilon).float()
    return (residual_norm * mask) / t.clamp(gt_norm, min=epsilon)

def log_magnitude_loss(x_in, x_out, hps, epsilon=1e-4):
    hps = DefaultSTFTValues(hps)
    spec_in = t.log(spec(squeeze(x_in.float()), hps) + epsilon)
    spec_out = t.log(spec(squeeze(x_out.float()), hps) + epsilon)
    return t.mean(t.abs(spec_in - spec_out))

# def load_audio(file, sr, offset, duration, mono=False):
#     # Librosa loads more filetypes than soundfile
#     x, _ = librosa.load(file, sr=sr, mono=mono, offset=offset/sr, duration=duration/sr)
#     if len(x.shape) == 1:
#         x = x.reshape((1, -1))
#     return x    

def save_wav(fname, aud, sr):
    # clip before saving?
    aud = t.clamp(aud, -1, 1).cpu().numpy()
    for i in list(range(aud.shape[0])):
        soundfile.write(f'{fname}/item_{i}.wav', aud[i], samplerate=sr, format='wav')

import sys

def def_tqdm(x):
    return tqdm(x, leave=True, file=sys.stdout, bar_format="{n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]")

def get_range(x):
    #if dist.get_rank() == 0:
    #    return def_tqdm(x)
    #else:
        return x

def init_logging(hps, local_rank, rank):
    logdir = f"{hps.local_logdir}/{hps.name}"
    if local_rank == 0:
        if not os.path.exists(logdir):
            os.makedirs(logdir)
        with open(logdir + 'argv.txt', 'w') as f:
            f.write(hps.argv + '\n')
        print("Logging to", logdir)
    logger = Logger(logdir, rank)
    metrics = Metrics()
    logger.add_text('hps', str(hps))
    return logger, metrics

def get_name(hps):
    name = ""
    for key, value in hps.items():
        name += f"{key}_{value}_"
    return name

def average_metrics(_metrics):
    metrics = {}
    for _metric in _metrics:
        for key, val in _metric.items():
            if key not in metrics:
                metrics[key] = []
            metrics[key].append(val)
    return {key: sum(vals)//len(vals) for key, vals in metrics.items()}

class Metrics:
    def __init__(self):
        self.sum = {}
        self.n = {}

    def update(self, tag, val, batch):
        # v is average value over batch
        # store total value and total batch, returns dist average
        sum = t.tensor(val * batch).float().cuda()
        n = t.tensor(batch).float().cuda()
        dist.all_reduce(sum)
        dist.all_reduce(n)
        sum = sum.item()
        n = n.item()
        self.sum[tag] = self.sum.get(tag, 0.0) + sum
        self.n[tag] = self.n.get(tag, 0.0) + n
        return sum / n

    def avg(self, tag):
        if tag in self.sum:
            return self.sum[tag] / self.n[tag]
        else:
            return 0.0

    def reset(self):
        self.sum = {}
        self.n = {}

class Logger:
    def __init__(self, logdir, rank):
        if rank == 0:
            from tensorboardX import SummaryWriter
            self.sw = SummaryWriter(f"{logdir}/logs")
        self.iters = 0
        self.rank = rank
        self.works = []
        self.logdir = logdir

    def step(self):
        self.iters += 1

    def flush(self):
        if self.rank == 0:
            self.sw.flush()

    def add_text(self, tag, text):
        if self.rank == 0:
            self.sw.add_text(tag, text, self.iters)

    def add_audios(self, tag, auds, sample_rate=22050, max_len=None, max_log=8):
        if self.rank == 0:
            for i in range(min(len(auds), max_log)):
                if max_len:
                    self.sw.add_audio(f"{i}/{tag}", auds[i][:max_len * sample_rate], self.iters, sample_rate)
                else:
                    self.sw.add_audio(f"{i}/{tag}", auds[i], self.iters, sample_rate)

    def add_audio(self, tag, aud, sample_rate=22050):
        if self.rank == 0:
            self.sw.add_audio(tag, aud, self.iters, sample_rate)

    def add_images(self, tag, img, dataformats="NHWC"):
        if self.rank == 0:
            self.sw.add_images(tag, img, self.iters, dataformats=dataformats)

    def add_image(self, tag, img):
        if self.rank == 0:
            self.sw.add_image(tag, img, self.iters)

    def add_scalar(self, tag, val):
        if self.rank == 0:
            self.sw.add_scalar(tag, val, self.iters)

    def get_range(self, loader):
        if self.rank == 0:
            self.trange = def_tqdm(loader)
        else:
            self.trange = loader
        return enumerate(self.trange)

    def close_range(self):
        if self.rank == 0:
            self.trange.close()

    def set_postfix(self, *args, **kwargs):
        if self.rank == 0:
            self.trange.set_postfix(*args, **kwargs)

    # For logging summaries of varies graph ops
    def add_reduce_scalar(self, tag, layer, val):
        if self.iters % 100 == 0:
            with t.no_grad():
                val = val.float().norm()/float(val.numel())
            work = dist.reduce(val, 0, async_op=True)
            self.works.append((tag, layer, val, work))

    def finish_reduce(self):
        for tag, layer, val, work in self.works:
            work.wait()
            if self.rank == 0:
                val = val.item()/dist.get_world_size()
                self.lw[layer].add_scalar(tag, val, self.iters)
        self.works = []

import torch as t
import torch.nn as nn

def dont_update(params):
    for param in params:
        param.requires_grad = False

def update(params):
    for param in params:
        param.requires_grad = True

def calculate_strides(strides, downs):
    return [stride ** down for stride, down in zip(strides, downs)]

def _loss_fn(loss_fn, x_target, x_pred, hps):
    if loss_fn == 'l1':
        return t.mean(t.abs(x_pred - x_target)) / hps.bandwidth['l1']
    elif loss_fn == 'l2':
        return t.mean((x_pred - x_target) ** 2) / hps.bandwidth['l2']
    elif loss_fn == 'linf':
        residual = ((x_pred - x_target) ** 2).reshape(x_target.shape[0], -1)
        values, _ = t.topk(residual, hps.linf_k, dim=1)
        return t.mean(values) / hps.bandwidth['l2']
    elif loss_fn == 'lmix':
        loss = 0.0
        if hps.lmix_l1:
            loss += hps.lmix_l1 * _loss_fn('l1', x_target, x_pred, hps)
        if hps.lmix_l2:
            loss += hps.lmix_l2 * _loss_fn('l2', x_target, x_pred, hps)
        if hps.lmix_linf:
            loss += hps.lmix_linf * _loss_fn('linf', x_target, x_pred, hps)
        return loss
    else:
        assert False, f"Unknown loss_fn {loss_fn}"

class VQVAE(nn.Module):
    def __init__(self, input_shape, levels, downs_t, strides_t,
                 emb_width, l_bins, mu, commit, spectral, multispectral,
                 multipliers=None, use_bottleneck=True, **block_kwargs):
        super().__init__()

        self.sample_length = input_shape[0]
        # print(input_shape)
        x_shape, x_channels = input_shape[:-1], input_shape[-1]
        self.x_shape = x_shape

        self.downsamples = calculate_strides(strides_t, downs_t)
        self.hop_lengths = np.cumprod(self.downsamples)
        self.z_shapes = z_shapes = [(x_shape[0] // self.hop_lengths[level],) for level in range(levels)]
        self.levels = levels

        if multipliers is None:
            self.multipliers = [1] * levels
        else:
            assert len(multipliers) == levels, "Invalid number of multipliers"
            self.multipliers = multipliers
        def _block_kwargs(level):
            this_block_kwargs = dict(block_kwargs)
            this_block_kwargs["width"] *= self.multipliers[level]
            this_block_kwargs["depth"] *= self.multipliers[level]
            return this_block_kwargs

        encoder = lambda level: Encoder(x_channels, emb_width, level + 1,
                                        downs_t[:level+1], strides_t[:level+1], **_block_kwargs(level))
        decoder = lambda level: Decoder(x_channels, emb_width, level + 1,
                                        downs_t[:level+1], strides_t[:level+1], **_block_kwargs(level))
        self.encoders = nn.ModuleList()
        self.decoders = nn.ModuleList()
        for level in range(levels):
            self.encoders.append(encoder(level))
            self.decoders.append(decoder(level))

        if use_bottleneck:
            self.bottleneck = Bottleneck(l_bins, emb_width, mu, levels)
        else:
            self.bottleneck = NoBottleneck(levels)

        self.downs_t = downs_t
        self.strides_t = strides_t
        self.l_bins = l_bins
        self.commit = commit
        self.spectral = spectral
        self.multispectral = multispectral

    def preprocess(self, x):
        # x: NTC [-1,1] -> NCT [-1,1]
        assert len(x.shape) == 3
        x = x.permute(0,2,1).float()
        return x

    def postprocess(self, x):
        # x: NTC [-1,1] <- NCT [-1,1]
        x = x.permute(0,2,1)
        return x

    def _decode(self, zs, start_level=0, end_level=None):
        # Decode
        if end_level is None:
            end_level = self.levels
        assert len(zs) == end_level - start_level
        xs_quantised = self.bottleneck.decode(zs, start_level=start_level, end_level=end_level)
        assert len(xs_quantised) == end_level - start_level

        # Use only lowest level
        decoder, x_quantised = self.decoders[start_level], xs_quantised[0:1]
        x_out = decoder(x_quantised, all_levels=False)
        x_out = self.postprocess(x_out)
        return x_out

    def decode(self, zs, start_level=0, end_level=None, bs_chunks=1):
        z_chunks = [t.chunk(z, bs_chunks, dim=0) for z in zs]
        x_outs = []
        for i in range(bs_chunks):
            zs_i = [z_chunk[i] for z_chunk in z_chunks]
            x_out = self._decode(zs_i, start_level=start_level, end_level=end_level)
            x_outs.append(x_out)
        return t.cat(x_outs, dim=0)

    def _encode(self, x, start_level=0, end_level=None):
        # Encode
        if end_level is None:
            end_level = self.levels
        x_in = self.preprocess(x)
        xs = []
        for level in range(self.levels):
            encoder = self.encoders[level]
            x_out = encoder(x_in)
            xs.append(x_out[-1])
        zs = self.bottleneck.encode(xs)
        return zs[start_level:end_level]

    def encode(self, x, start_level=0, end_level=None, bs_chunks=1):
        x_chunks = t.chunk(x, bs_chunks, dim=0)
        zs_list = []
        for x_i in x_chunks:
            zs_i = self._encode(x_i, start_level=start_level, end_level=end_level)
            zs_list.append(zs_i)
        zs = [t.cat(zs_level_list, dim=0) for zs_level_list in zip(*zs_list)]
        return zs

    def sample(self, n_samples):
        zs = [t.randint(0, self.l_bins, size=(n_samples, *z_shape), device='cuda') for z_shape in self.z_shapes]
        return self.decode(zs)

    def forward(self, x, hps, loss_fn='l1'):
        metrics = {}

        N = x.shape[0]

        # Encode/Decode
        x_in = self.preprocess(x)
        xs = []
        for level in range(self.levels):
            encoder = self.encoders[level]
            x_out = encoder(x_in)
            xs.append(x_out[-1])

        zs, xs_quantised, commit_losses, quantiser_metrics = self.bottleneck(xs)
        x_outs = []
        for level in range(self.levels):
            decoder = self.decoders[level]
            x_out = decoder(xs_quantised[level:level+1], all_levels=False)
            assert_shape(x_out, x_in.shape)
            x_outs.append(x_out)

        # Loss
        def _spectral_loss(x_target, x_out, hps):
            if hps.use_nonrelative_specloss:
                sl = spectral_loss(x_target, x_out, hps) / hps.bandwidth['spec']
            else:
                sl = spectral_convergence(x_target, x_out, hps)
            sl = t.mean(sl)
            return sl

        def _multispectral_loss(x_target, x_out, hps):
            sl = multispectral_loss(x_target, x_out, hps) / hps.bandwidth['spec']
            sl = t.mean(sl)
            return sl

        recons_loss = t.zeros(()).to(x.device)
        spec_loss = t.zeros(()).to(x.device)
        multispec_loss = t.zeros(()).to(x.device)
        x_target = audio_postprocess(x.float(), hps)

        for level in reversed(range(self.levels)):
            x_out = self.postprocess(x_outs[level])
            x_out = audio_postprocess(x_out, hps)
            this_recons_loss = _loss_fn(loss_fn, x_target, x_out, hps)
            this_spec_loss = _spectral_loss(x_target, x_out, hps)
            this_multispec_loss = _multispectral_loss(x_target, x_out, hps)
            metrics[f'recons_loss_l{level + 1}'] = this_recons_loss
            metrics[f'spectral_loss_l{level + 1}'] = this_spec_loss
            metrics[f'multispectral_loss_l{level + 1}'] = this_multispec_loss
            recons_loss += this_recons_loss
            spec_loss += this_spec_loss
            multispec_loss += this_multispec_loss

        commit_loss = sum(commit_losses)
        loss = recons_loss + self.spectral * spec_loss + self.multispectral * multispec_loss + self.commit * commit_loss

        with t.no_grad():
            sc = t.mean(spectral_convergence(x_target, x_out, hps))
            l2_loss = _loss_fn("l2", x_target, x_out, hps)
            l1_loss = _loss_fn("l1", x_target, x_out, hps)
            linf_loss = _loss_fn("linf", x_target, x_out, hps)

        quantiser_metrics = average_metrics(quantiser_metrics)

        metrics.update(dict(
            recons_loss=recons_loss,
            spectral_loss=spec_loss,
            multispectral_loss=multispec_loss,
            spectral_convergence=sc,
            l2_loss=l2_loss,
            l1_loss=l1_loss,
            linf_loss=linf_loss,
            commit_loss=commit_loss,
            **quantiser_metrics))

        for key, val in metrics.items():
            metrics[key] = val.detach()

        return x_out, loss, metrics

"""## **HPS**"""

HPARAMS_REGISTRY = {}

class Hyperparams(dict):
    def __getattr__(self, attr):
        return self[attr]

    def __setattr__(self, attr, value):
        self[attr] = value

def setup_hparams(hparam_set_names, kwargs):
    H = Hyperparams()
    if not isinstance(hparam_set_names, tuple):
        hparam_set_names = hparam_set_names.split(",")
    hparam_sets = [HPARAMS_REGISTRY[x.strip()] for x in hparam_set_names if x] + [kwargs]
    for k, v in DEFAULTS.items():
        H.update(v)
    for hps in hparam_sets:
        for k in hps:
            if k not in H:
                raise ValueError(f"{k} not in default args")
        H.update(**hps)
    H.update(**kwargs)
    return H

# Model hps
vq = Hyperparams(
    sr = 11000,
    levels = 1,
    downs_t = (5, 5),
    strides_t = (2, 2),
    emb_width = 64,
    l_bins = 1024,
    l_mu = 0.99,
    commit = 0.02,
    spectral = 0.0,
    multispectral = 1.0,
    loss_fn = 'l2',
    width = 32,
    depth = 4,
    m_conv = 1.0,
    dilation_growth_rate = 3,
    revival_threshold=1.0,
    hvqvae_multipliers = None,
    lmix_l1=0.0,
    lmix_l2 = 1.0,
    lmix_linf=0.02,
    linf_k=2,
    use_bottleneck=True,
    dilation_cycle=None,
    vqvae_reverse_decoder_dilation=True,
    sample_length = 12.0*11000,
    restore_vqvae='./drive/MyDrive/VQVAE-trans/vqvae-checkpoint-4/checkpoint_step_8001.pth.tar',
    lr=0.0003,
    clip=1.0,
    beta1=0.9,
    beta2=0.999,
    ignore_grad_norm=0,
    weight_decay=0.0,
    eps=1e-08,
    lr_warmup=100.0,
    lr_decay=10000000000.0,
    lr_gamma=1.0,
    lr_scale=1.0,
    lr_use_linear_decay=False,
    lr_start_linear_decay=0,
    lr_use_cosine_decay=False,
    save_iters = 1000,
    save = True,
    aug_blend=False,
    n_inps=1,
    n_hops=2,
    n_segment=1,
    n_total_segment=1,
    n_segment_each=1,
    prime_chunks=4,
    sample_hop_length=30000,
    max_silence_pad_length=0,
    ignore_boundaries=False,
    use_nonrelative_specloss=True,
    multispec_loss_n_fft=(2048,1024,512),
    multispec_loss_hop_length=(240,120,50),
    multispec_loss_window_size=(1200,600,240),
)

def load_checkpoint(path):
    restore = path
    if restore[:5] == 'gs://':
        gs_path = restore
        local_path = os.path.join(os.path.expanduser("~/.cache"), gs_path[5:])
        if dist.get_rank() % 8 == 0:
            print("Downloading from gce")
            if not os.path.exists(os.path.dirname(local_path)):
                os.makedirs(os.path.dirname(local_path))
            if not os.path.exists(local_path):
                download(gs_path, local_path)
        restore = local_path
    checkpoint = t.load(restore, map_location=t.device('cpu'))
    print("Restored from {}".format(restore))
    return checkpoint

def save_checkpoint(i,name, model, opt, metrics, hps):
    with t.no_grad():
        prefix = './drive/My Drive/VQVAE-trans/vqvae-checkpoint-2/'
        # name = time.strftime(prefix + '%m%d_%H_%M_%S.pth.tr')
        save_hps = {**hps}
        save_hps = {k: v for k,v in save_hps.items() if k not in ['metadata_v2','metadata_v3', 'alignments', 'lyric_processor', 'midi_processor']}
        t.save({'hps': save_hps,
                'model': model.state_dict(), # should also save bottleneck k's as buffers
                'opt': opt.state_dict() if opt is not None else None,
                'step': i,
                **metrics},f'{prefix}/checkpoint_{name}.pth.tar')
    return

def restore_model(hps, model, checkpoint_path):
    model.step = 0
    if checkpoint_path != '':
        checkpoint = load_checkpoint(checkpoint_path)
        # checkpoint_hps = Hyperparams(**checkpoint['hps'])
        # for k in set(checkpoint_hps.keys()).union(set(hps.keys())):
        #     if checkpoint_hps.get(k, None) != hps.get(k, None):
        #         print(k, "Checkpoint:", checkpoint_hps.get(k, None), "Ours:", hps.get(k, None))
        checkpoint['model'] = {k[7:] if k[:7] == 'module.' else k: v for k, v in checkpoint['model'].items()}
        model.load_state_dict(checkpoint['model'])
        if 'step' in checkpoint: model.step = checkpoint['step']

def restore_opt(opt, shd, checkpoint_path):
    if not checkpoint_path:
        return
    checkpoint = load_checkpoint(checkpoint_path)
    if "opt" in checkpoint:
        opt.load_state_dict(checkpoint['opt'])
    if "step" in checkpoint:
        shd.step(checkpoint['step'])

def make_vqvae(hps, device='cuda',train = True):
    block_kwargs = dict(width=hps.width, depth=hps.depth, m_conv=hps.m_conv,
                        dilation_growth_rate=hps.dilation_growth_rate,
                        dilation_cycle=hps.dilation_cycle,
                        reverse_decoder_dilation=hps.vqvae_reverse_decoder_dilation)

    if not hps.sample_length:
        assert hps.sample_length_in_seconds != 0
        downsamples = calculate_strides(hps.strides_t, hps.downs_t)
        top_raw_to_tokens = np.prod(downsamples)
        hps.sample_length = (hps.sample_length_in_seconds * hps.sr // top_raw_to_tokens) * top_raw_to_tokens
        print(f"Setting sample length to {hps.sample_length} (i.e. {hps.sample_length/hps.sr} seconds) to be multiple of {top_raw_to_tokens}")

    vqvae = VQVAE(input_shape=(hps.sample_length,1), levels=hps.levels, downs_t=hps.downs_t, strides_t=hps.strides_t,
                  emb_width=hps.emb_width, l_bins=hps.l_bins,
                  mu=hps.l_mu, commit=hps.commit,
                  spectral=hps.spectral, multispectral=hps.multispectral,
                  multipliers=hps.hvqvae_multipliers, use_bottleneck=hps.use_bottleneck,
                  **block_kwargs)

    vqvae = vqvae.to(device)
    restore_model(hps, vqvae, hps.restore_vqvae)
    if train:
        print(f"Loading vqvae in train mode")
        if hps.restore_vqvae != '':
            print("Reseting bottleneck emas")
            for level, bottleneck in enumerate(vqvae.bottleneck.level_blocks):
                num_samples = hps.sample_length
                downsamples = calculate_strides(hps.strides_t, hps.downs_t)
                raw_to_tokens = np.prod(downsamples[:level + 1])
                num_tokens = (num_samples // raw_to_tokens)
                bottleneck.restore_k(num_tokens=num_tokens, threshold=hps.revival_threshold)
    else:
        print(f"Loading vqvae in eval mode")
        vqvae.eval()
        freeze_model(vqvae)
    return vqvae

"""## **Make VQ-VAE**"""

vqvae = make_vqvae(vq, device = t.device("cuda"),train=False)

vqvae.z_shapes

"""## **Training VQ-VAE**"""

# def get_ema(model, hps):
#     mu = hps.mu or (1. - (hps.bs * hps.ngpus/8.)/1000)
#     ema = None
#     if hps.ema and hps.train:
#         if hps.cpu_ema:
#             if dist.get_rank() == 0:
#                 print("Using CPU EMA")
#             ema = CPUEMA(model.parameters(), mu=mu, freq=hps.cpu_ema_freq)
#         elif hps.ema_fused:
#             ema = FusedEMA(model.parameters(), mu=mu)
#         else:
#             ema = EMA(model.parameters(), mu=mu)
#     return ema

def get_lr_scheduler(opt, hps):
    def lr_lambda(step):
        if hps.lr_use_linear_decay:
            lr_scale = hps.lr_scale * min(1.0, step / hps.lr_warmup)
            decay = max(0.0, 1.0 - max(0.0, step - hps.lr_start_linear_decay) / hps.lr_decay)
            if decay == 0.0:
                if dist.get_rank() == 0:
                    print("Reached end of training")
            return lr_scale * decay
        else:
            return hps.lr_scale * (hps.lr_gamma ** (step // hps.lr_decay)) * min(1.0, step / hps.lr_warmup)

    shd = t.optim.lr_scheduler.LambdaLR(opt, lr_lambda)

    return shd

def get_optimizer(model, hps):
    # Optimizer
    betas = (hps.beta1, hps.beta2)
    opt = FusedAdam(model.parameters(), lr=hps.lr, weight_decay=hps.weight_decay, betas=betas, eps=hps.eps)

    # lr scheduler
    shd = get_lr_scheduler(opt, hps)
    restore_opt(opt, shd, restore_path)

    # fp16 dynamic loss scaler
    scalar = None
    if hps.fp16:
        rank = dist.get_rank()
        local_rank = rank % 8
        scalar = LossScalar(hps.fp16_loss_scale, scale_factor=2 ** (1./hps.fp16_scale_window))
        if local_rank == 0: print(scalar.__dict__)

    zero_grad(model)
    return opt, shd, scalar

def backward(loss, params, scalar, fp16):
    # Perform backward
    if not fp16:
        scale = 1.0
        loss.backward()
        gn = grad_norm(params, scale)
        return loss, scale, gn, False, False
    else:
        scale = scalar.get_scale()
        loss = (loss.float())*scale
        overflow_loss = check_overflow(loss.item())
        overflow_loss = allreduce(int(overflow_loss), op=dist.ReduceOp.MAX) > 0
        if not overflow_loss:
            loss.backward()
            gn = grad_norm(params, scale)
            overflow_grad = check_overflow(gn)
            overflow_grad = allreduce(int(overflow_grad), op=dist.ReduceOp.MAX) > 0
            scalar.update_scale(overflow_grad)
        else:
            gn = 0.0
            overflow_grad = True
        loss = (loss.detach().float()) / scale # Should delete computation graph for overflow
        if logger.rank == 0:
            if loss > 12.: print(f"\nWarning. Loss is {loss}")
            if overflow_loss: print(f"\nOverflow in forward. Loss {loss}, lgscale {np.log2(scale)}. Skipping batch completely (no backward, scale update)")
            elif overflow_grad: print(f"\nOverflow in backward. Loss {loss}, grad norm {gn}, lgscale {np.log2(scale)}, new lgscale {np.log2(scalar.get_scale())}")
        return loss, scale, gn, overflow_loss, overflow_grad
# Automatic loss scaling
class LossScalar(object):
    def __init__(self,
                 loss_scale,
                 init_scale=2. ** 16,
                 scale_factor=2. ** (1. / 1000),
                 scale_window=1):
        if loss_scale == None:
            # Use dynamic loss scaling
            self.dynamic = True
            self.loss_scale = init_scale
        else:
            self.dynamic = False
            self.loss_scale = loss_scale
        self.max_loss_scale = 2.**24
        self.scale_factor = scale_factor
        self.scale_window  = scale_window
        self.unskipped = 0
        self.overflow = False

    def get_scale(self):
        return self.loss_scale

    def update_scale(self, overflow):
        if overflow and self.dynamic:
            self.loss_scale /= 2.
            self.unskipped = 0
        else:
            self.unskipped += 1

        if self.unskipped == self.scale_window and self.dynamic:
            self.loss_scale = min(self.max_loss_scale, self.loss_scale * self.scale_factor)
            self.unskipped = 0


def check_overflow(val):
    return (val == float('inf')) or (val == -float('inf')) or (val != val)

def grad_norm(params, scale, flat=False):
    params = list(params)
    if flat:
        # Faster but more memory
        fp16_grads = [p.grad for p in params if p.grad is not None and p.data.dtype == t.float16]
        fp16_norm = 0.0 if len(fp16_grads) == 0 else float(_flatten_dense_tensors(fp16_grads).norm(p=2, dtype=t.float32))
        fp32_grads = [p.grad for p in params if p.grad is not None and p.data.dtype != t.float16]
        fp32_norm = 0.0 if len(fp32_grads) == 0 else float(_flatten_dense_tensors(fp32_grads).norm(p=2))
        grad_norm = (fp16_norm**2 + fp32_norm**2)**0.5
    else:
        # Slightly slower but less memory
        grad_norm = 0.0
        for p in params:
            if p.grad is not None:
                grad_norm += p.grad.norm(p=2, dtype=t.float32)**2
        grad_norm = float(grad_norm**0.5)
    return grad_norm / scale

def clipped_grad_scale(grad_norm, max_grad_norm, scale):
    clip = grad_norm / max_grad_norm
    if clip > 1:
        scale = clip * scale
    return scale

import torch.optim as optim
# vq.bandwidth = calculate_bandwidth(dataset_reload, vq, duration=len(dataset_reload)*24.0);

# bandwidth_dict = vq.bandwidth;
# try: 
#    filename = '/content/drive/My Drive/EMOTION/VQVAE-trans/Pytorch_dataset/bandwidth2.pkl'
#    filehandler = open(filename, 'wb')
#    pickle.dump(bandwidth_dict, filehandler) 
#    print('saved') 
# except: 
#    print("Unable to write to file")

try: 
    filename = './drive/My Drive/EMOTION/VQVAE-trans/Pytorch_dataset/bandwidth3.pkl'
    filehandler = open(filename, 'rb')
    bandwidth_dict = pickle.load(filehandler) 
    print(bandwidth_dict)
    print('retrieved')  
except: 
    print("Unable to read file")

vq.bandwidth = bandwidth_dict

model = vqvae

# Setup opt, ema and distributed_model.
opt = optim.Adam(model.parameters(), lr=0.0003, amsgrad=False)
metrics = Metrics()
shd = get_lr_scheduler(opt,vq)

fp16=False
fp16_params=False
fp16_loss_scale=None
fp16_scale_window=1000.0
fp16_opt=False

scalar = LossScalar(None, scale_factor=2 ** (1./fp16_scale_window))
# ema = get_ema(model, optimizer)

# logger, metrics = init_logging(opt, local_rank, rank)
# logger.iters = model.step

def train(model, opt, shd, scalar, metrics, train_loader, hps):
    model.train()

    for i, x in enumerate(train_loader):
        if isinstance(x, (tuple, list)):
            x, y = x
        else:
            y = None

        x = x.to('cuda', non_blocking=True)
        if y is not None:
            y = y.to('cuda', non_blocking=True)

        # print(x.shape)
        # fname = 'doom'
        # soundfile.write(f'{fname}/item_{i}.wav', x[1], samplerate=hps.sr, format='wav')

        x_in = x = audio_preprocess(x, hps)
        #log_input_output = (logger.iters % hps.save_iters == 0)
        forw_kwargs = dict(loss_fn=hps.loss_fn, hps=hps)

        # Forward
        x_out, loss, _metrics = model(x, **forw_kwargs)
        del x

        


        # Backward
        loss, scale, grad_norm, overflow_loss, overflow_grad = backward(loss=loss, params=list(model.parameters()),
                                                                         scalar=scalar, fp16=False)
        # Skip step if overflow
        # grad_norm = allreduce(grad_norm, op=dist.ReduceOp.MAX)
        # if overflow_loss or overflow_grad or grad_norm > hps.ignore_grad_norm > 0:
        #     zero_grad(model)
        #     continue

        # Step opt. Divide by scale to include clipping and fp16 scaling
        # logger.step()
        opt.step()
        zero_grad(model)
        lr = hps.lr if shd is None else shd.get_lr()[0]
        if shd is not None: shd.step()
        # if ema is not None: ema.step()
        next_lr = hps.lr if shd is None else shd.get_lr()[0]
        finished_training = (next_lr == 0.0)

        # Logging
        for key, val in _metrics.items():
            _metrics[key] = val.item()
        _metrics["loss"] = loss = loss.item() * 1 # Make sure to call to free graph
        _metrics["gn"] = grad_norm
        _metrics["lr"] = lr
        _metrics["lg_loss_scale"] = np.log2(scale)

        # Average and log
        # for key, val in _metrics.items():
        #     _metrics[key] = metrics.update(key, val, x.shape[0])
        #     if logger.iters % hps.log_steps == 0:
        #         logger.add_scalar(key, _metrics[key])

        # Save checkpoint
        with t.no_grad():
            if hps.save and (i % hps.save_iters == 1 or finished_training):
                # if ema is not None: ema.swap()
                model.eval()
                name = f'step_{i}'
                save_checkpoint(i,name, model, opt, dict(step=id), hps)
                model.train()
                # if ema is not None: ema.swap()

        # Sample
        # with t.no_grad():
        #     if (logger.iters % 12000) in list(range(1, 1 + hps.iters_before_update)) or finished_training:
        #         if hps.prior:
        #             sample_prior(orig_model, ema, logger, x_in, y, hps)

        # Input/Output
        # with t.no_grad():
        #     if log_input_output:
        #         log_inputs(orig_model, logger, x_in, y, x_out, hps)

        #displaying loss have to optimize 
        # for print_key, key in _print_keys.items():
        #print('--------------------------------------------------------------------------------------------------------------------------------------------------')
        for key, val in _metrics.items():
            if(key == 'recons_loss_l1' or key == 'spectral_loss_l1' or key == 'multispectral_loss_l1' or key == 'l2_loss' or key == 'commit_loss'):
              print(key,':',val,",",end = " ")
        print()
        for key, val in _metrics.items():
            if(key == 'loss' or key == 'entropy'):
              print(key,':',val,end = " ")
        print()
        print('--------------------------------------------------------------------------------------------------------------------------------------------------')

        
        #logger.set_postfix(**{print_key:_metrics[key] for print_key, key in _print_keys.items()})
        if finished_training:
            #dist.barrier()
            exit()
    #logger.close_range()
    return {key: metrics.avg(key) for key in _metrics.keys()}

vqvae.z_shapes

# n_iterations = 100

# for epoch in range(n_iterations):
#     #training loop
#     print('--------------------------------------------------------------------------------------------------------------------------------------------------')
#     print('iter no : ',epoch)
#     metrics.reset()
#     train_metrics = train(model, opt, shd, scalar, metrics, train_loader, vq)
#     train_metrics['epoch'] = epoch
#     print('--------------------------------------------------------------------------------------------------------------------------------------------------')

"""## **Evaluation VQ-VAE**"""

def save_to_wav(fname, aud, sr,j):
    # clip before saving?
    aud = aud.detach()
    aud = t.clamp(aud, -1, 1).cpu().numpy()
    print(aud.shape)
    for i in list(range(aud.shape[0])):
        soundfile.write(f'{fname}/item_{j}.wav', aud[i], samplerate=sr, format='wav')

def eval(model, opt,metrics, train_loader, hps):
    model.eval()
    with t.no_grad():
      for i, x in enumerate(train_loader):
          if isinstance(x, (tuple, list)):
              x, y = x
          else:
              y = None

          x = x.to('cuda', non_blocking=True)
          if y is not None:
              y = y.to('cuda', non_blocking=True)

          x_in = x = audio_preprocess(x, hps)
          #log_input_output = (logger.iters % hps.save_iters == 0)
          forw_kwargs = dict(loss_fn=hps.loss_fn, hps=hps)

          # Forward
          x_out, loss, _metrics = model(x, **forw_kwargs)

          save_to_wav('./drive/My Drive/VQVAE-trans/output-VQVAE',x_out,11000,i)

          del x

          # Logging
          for key, val in _metrics.items():
              _metrics[key] = val.item()
          _metrics["loss"] = loss = loss.item() * 1 # Make sure to call to free graph
          
          if(i == 5):
            break          #processing only 10 inputs 
          #logger.set_postfix(**{print_key:_metrics[key] for print_key, key in _print_keys.items()})
    #logger.close_range()
    return {key: metrics.avg(key) for key in _metrics.keys()}

# pred = eval(model, opt, metrics, train_loader, vq)

"""## **Transformer - Reformer**

## **Training**
"""

def add_argument():
    parser=argparse.ArgumentParser(description='mastero')

    parser.add_argument('--with_cuda', default=True, action='store_true',
                        help='use CPU in case there\'s no GPU support')
    parser.add_argument('--use_ema', default=False, action='store_true',
                        help='whether use exponential moving average')
    parser.add_argument('-b', '--batch_size', default=1, type=int,
                        help='mini-batch size (default: 1)')
    parser.add_argument('-e', '--epochs', default=30, type=int,
                        help='number of total epochs (default: 30)')
    parser.add_argument('--local_rank', type=int, default=-1,
                       help='local rank passed from distributed launcher')

    parser = deepspeed.add_config_arguments(parser)
    args=parser.parse_args()
    return args

NUM_BATCHES = int(1e5)
NUM_ITERATIONS = 1051
BATCH_SIZE = 1
GRADIENT_ACCUMULATE_EVERY = 4
LEARNING_RATE = 1e-4
SAVE_EVERY  = 100
GENERATE_EVERY  = 500
GENERATE_LENGTH = 20000
SEQ_LENGTH = 4096
save_model = True
load_model = False

def save_checkpoint(state,epoch):
    print("=> Saving checkpoint")
    prefix = './drive/MyDrive/audio_VAE/Routing_checkpoint_2'
    filename = f'{prefix}/checkpoint_{epoch}.pth.tar'
    t.save(state, filename)


def load_checkpoint_transformer(checkpoint, model, optimizer):
    print("=> Loading checkpoint")
    model.load_state_dict(checkpoint["state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer"])

model = RoutingTransformerLM(
    num_tokens = 1024,
    dim = 768,
    depth = 6,
    max_seq_len = SEQ_LENGTH,
    heads = 8,
    causal = True,
    window_size = 128,
    n_local_attn_heads = (8, 8, 8, 4, 4, 4)
)

model = AutoregressiveWrapper(model)
model.cuda()
# # optimizer
# optim = t.optim.Adam(model.parameters(), lr=LEARNING_RATE)

cmd_args = add_argument()
model_engine, optimizer, trainloader, _ = deepspeed.initialize(args=cmd_args, model=model, model_parameters=model.parameters(),  training_data=train_dataset)

if load_model:
    load_checkpoint_transformer(t.load("./drive/MyDrive/audio_VAE/Routing_checkpoint_2/checkpoint_700.pth.tar"), model_engine, optimizer)

# training
 
for epoch in range(NUM_BATCHES):
    model_engine.train()
    loss_total = 0
    count = 0
    
    # print(f'epoch : {epoch}')
 
    for i in tqdm(range(NUM_ITERATIONS)):
        song = next(iter(train_loader))
        song = song.to('cuda', non_blocking=True)
        x_in = song = audio_preprocess(song, vq)
        rescale = lambda z_shape: (z_shape[0]*4125//vqvae.z_shapes[0][0],)
        z_shapes = [rescale(z_shape) for z_shape in vqvae.z_shapes]
        z = vqvae.encode(song, start_level=0, end_level=len(z_shapes), bs_chunks=1)
 
        z = z[0]
 
        # z = z[0].reshape(hps.n_batch, hps.n_timesteps+62)
        
        seqs_source = [z_batch for z_batch in z[:, :]]
        #seqs_target = [z_batch for z_batch in z[:, 62:]]
        source = t.cat([s[None, :] for s in seqs_source ], dim=0).to(t.long)
        #target = t.cat([s[None, :] for s in seqs_target ], dim=0).to(t.long)
 
        #host_arr = source.cpu()
        #label = target.cpu()
        x_data = source
        #y_data = label.numpy()
 
        start = 0
 
        # for __ in range(GRADIENT_ACCUMULATE_EVERY):
        #     loss = model(next(train_loader), return_loss = True)
        #     loss.backward()
 
        # print(f'training loss: {loss.item()}')
        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        # optim.step()
        # optim.zero_grad()
 
        for ii in range(3):
          x = x_data[:,start:start+SEQ_LENGTH] 
          # y = x_data[:,start+1:start+1+SEQ_LENGTH]
          # loss = model(x, return_loss = True)
          # loss.backward()

          data = data.to(model_engine.local_rank)
          loss = model_engine(data, return_loss = True)
          loss_total += loss.item()
        
          model_engine.backward(loss)
          model_engine.step()
 
          start += 10
          count += 1
        
        if(i % 10 == 0): 
          print(f'\nloss : {loss.item()} :: epoch : {epoch}')
          print(f'training loss: {loss_total/(count)} :: epoch : {epoch}')
 
        if(i % 500 == 0):
          if save_model:
            checkpoint = {
              "state_dict": model_engine.state_dict(),
              "optimizer": optimizer.state_dict(),
              }
            save_checkpoint(checkpoint,i)
      
 
    if save_model:
          checkpoint = {
            "state_dict": model_engine.state_dict(),
            "optimizer": optimizer.state_dict(),
            }
          save_checkpoint(checkpoint,i)
    
    # if i % VALIDATE_EVERY == 0:
    #     model.eval()
    #     with torch.no_grad():
    #         loss = model(next(val_loader), return_loss = True)
    #         print(f'validation loss: {loss.item()}')
 
    # if i % GENERATE_EVERY == 0:
    #     model.eval()
    #     inp = random.choice(val_dataset)[:-1]
    #     prime = decode_tokens(inp)
    #     print(f'%s \n\n %s', (prime, '*' * 100))
 
    #     sample = model.generate(inp, GENERATE_LENGTH)
    #     output_str = decode_tokens(sample)
    #     print(output_str)

z2.shape

model.eval()
song = next(iter(train_loader))
song = song.to('cuda', non_blocking=True)
x_in = song = audio_preprocess(song, vq)
rescale = lambda z_shape: (z_shape[0]*4125//vqvae.z_shapes[0][0],)
z_shapes = [rescale(z_shape) for z_shape in vqvae.z_shapes]
z = vqvae.encode(song, start_level=0, end_level=len(z_shapes), bs_chunks=1)
z = z[0]
# z = z[0].reshape(hps.n_batch, hps.n_timesteps+62)
seqs_source = [z_batch for z_batch in z[:, :]]
#seqs_target = [z_batch for z_batch in z[:, 62:]]
source = t.cat([s[None, :] for s in seqs_source ], dim=0).to(t.long)
#target = t.cat([s[None, :] for s in seqs_target ], dim=0).to(t.long)
#host_arr = source.cpu()
#label = target.cpu()
x_data = source
#y_data = label.numpy()
start = 0
# prime = decode_tokens(inp)
# print(f'%s \n\n %s', (prime, '*' * 100))
x = x_data[:,start:start+SEQ_LENGTH] 

sample = model.generate(x, GENERATE_LENGTH)
# output_str = decode_tokens(sample)
# print(output_str)

sample.shape

predicted_sentence = vqvae.decode([sample])

def save_to_wav(fname, aud, sr,j):
    # clip before saving?
    aud = aud.detach()
    aud = t.clamp(aud, -1, 1).cpu().numpy()
    print(aud.shape)
    for i in list(range(aud.shape[0])):
        soundfile.write(f'{fname}/item_{j}.wav', aud[i], samplerate=sr, format='wav')

save_to_wav('./drive/My Drive/audio_VAE/output_transformer',predicted_sentence,11000,7)









"""## **Evaluation**"""

########################################
# search strategy: temperature (re-shape)
########################################
def temperature(logits, temperature):
        probs = np.exp(logits / temperature) / np.sum(np.exp(logits / temperature))
        return probs


########################################
# search strategy: nucleus (truncate)
########################################
def nucleus(probs, p):
    
    probs /= sum(probs)
    sorted_probs = np.sort(probs)[::-1]
    sorted_index = np.argsort(probs)[::-1]
    cusum_sorted_probs = np.cumsum(sorted_probs)
    after_threshold = cusum_sorted_probs > p
    if sum(after_threshold) > 0:
        last_index = np.where(after_threshold)[0][-1]
        candi_index = sorted_index[:last_index]
    else:
        candi_index = sorted_index[:3] # just assign a value
    candi_probs = [probs[i] for i in candi_index]
    candi_probs /= sum(candi_probs)
    word = np.random.choice(candi_index, size=1, p=candi_probs)[0]
  
    return word

def predict():

    with tf.Session(config=config) as sess:

        saver = tf.train.Saver(max_to_keep=5)

        if restore:
              saver.restore(sess, checkpoint)
        else:
              sess.run(tf.global_variables_initializer())
        
        if mpi_size > 1:
            # sync variables initialized on rank 0 to all other ranks
            sess.run(bs.sync_variables_op(mpi_rank))

        for i, song in enumerate(train_loader):
            song = song.to('cuda', non_blocking=True)
            x_in = song = audio_preprocess(song, vq)
            rescale = lambda z_shape: (z_shape[0]*22000//vqvae.z_shapes[0][0],)
            z_shapes = [rescale(z_shape) for z_shape in vqvae.z_shapes]
            z = vqvae.encode(song, start_level=0, end_level=len(z_shapes), bs_chunks=1)

            z = z[0]

            # z = z[0].reshape(hps.n_batch, hps.n_timesteps+62)
            
            seqs_source = [z_batch for z_batch in z[:, :]]
            #seqs_target = [z_batch for z_batch in z[:, 62:]]
            source = t.cat([s[None, :] for s in seqs_source ], dim=0).to(t.long)
          
            #final array of notes
            final_prediction = list(source.cpu().numpy())

            host_arr = source.cpu()
            #label = target.cpu()
            x_data = host_arr.numpy()
            #y_data = label.numpy()

            x = x_data[:,start:start+hps.n_timesteps] 
            y = x_data[:,start+1:start+1+hps.n_timesteps]

            while len(final_prediction) < 4000:
                
                logits,loss = sess.run([test_logits, valid_loss], feed_dict={X: x, Y: y})
                
                # print(logits.shape)

                last_logit = logits[-1,:]
                last_logit = last_logit.reshape(1,-1)

                # print(last_logit)

                y_pred = tf.argmax(tf.nn.softmax(last_logit,axis=-1),axis=-1)
                #y_pred = temperature(logits,1)
                #word = nucleus(probs=y_pred, p=0.90)
                y_pred = y_pred.eval(session=tf.compat.v1.Session())  
                
                # print(y_pred.shape)

                final_prediction.append(y_pred[0])

                #prediction = y_pred.reshape(1, len(y_pred))
                #prediction = t.tensor(prediction)
                #prediction = prediction.to('cuda', non_blocking=True)

                #host_arr = prediction.cpu()
                x = np.append(x,y_pred)

                # print(x)
                x = x[1:]
                print(len(final_prediction))

                x = x.reshape(1, len(x))
            
            break
    return final_prediction

pred = predict()

print(pred)
pred = np.array(pred)

prediction = pred.reshape(1, len(pred))
prediction = t.tensor(prediction)
prediction = prediction.to('cuda', non_blocking=True)
predicted_sentence = vqvae.decode([prediction])

def save_to_wav(fname, aud, sr,j):
    # clip before saving?
    aud = aud.detach()
    aud = t.clamp(aud, -1, 1).cpu().numpy()
    print(aud.shape)
    for i in list(range(aud.shape[0])):
        soundfile.write(f'{fname}/item_{j}.wav', aud[i], samplerate=sr, format='wav')

save_to_wav('/content/drive/My Drive/audio_VAE/output_transformer',predicted_sentence,22000,3)

"""## **Sanity check**"""

from tqdm import tqdm
n_ctx = 2048
 
for i, song in enumerate(tqdm(train_loader)):

    song = song.to('cuda', non_blocking=True)
    x_in = song = audio_preprocess(song, vq)
    rescale = lambda z_shape: (z_shape[0]*22000//vqvae.z_shapes[0][0],)
    z_shapes = [rescale(z_shape) for z_shape in vqvae.z_shapes]
    z = vqvae.encode(song, start_level=0, end_level=len(z_shapes), bs_chunks=1)

    z = z[0].reshape(8, n_ctx+702)
    


    seqs_source = [z_batch for z_batch in z[:, 701:-1]]
    seqs_target = [z_batch for z_batch in z[:, 702:]]
    source = t.cat([s[None, :] for s in seqs_source ], dim=0).to(t.long)


    target = t.cat([s[None, :] for s in seqs_target ], dim=0).to(t.long)

    print(source[0][:50])
    print(target[0][:50])
   
    break



